{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-3 review","provenance":[],"authorship_tag":"ABX9TyPlLbWJnHH9U1L/SFn36dEP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 정훈\n","\n","3-3 특성 공학과 규제\n","\n","\n","* 데이터 세팅 - 판다스 라이브러리\n","\n","```python\n","#판다스: 데이터 분석 라이브러리\n","import pandas as pd\n","\n","#length, height, width\n","df=pd.read_csv(\"https://bit.ly/perch_csv_data\")\n","#to_numpy 함수로 numpy 배열로 바꾼다.\n","perch_full=df.to_numpy()\n","print(perch_full)\n","```\n","    [[ 8.4   2.11  1.41]\n","    [13.7   3.53  2.  ]\n","    [15.    3.82  2.43]\n","    [16.2   4.59  2.63]\n","    [17.4   4.59  2.94]\n","    [18.    5.22  3.32]\n","    ...\n","\n",":::info 판다스 라이브러리\n","pandas 라이브러리에 read_csv 함수에게 csv파일의 url을 넘겨주면 데이터 프레임을 만들 수 있다.(이후 numpy 배열로의 변환이 필요)\n",":::\n","\n","<br/>\n","\n","* target 데이터 설정 및 테스트, 훈련 세트 설정\n","\n","```python\n","import numpy as np\n","\n","#타겟 데이터 설정\n","perch_weight=np.array([5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0, 110.0,\n","       115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0, 130.0,\n","       150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0, 197.0,\n","       218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0, 514.0,\n","       556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0, 820.0,\n","       850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0, 1000.0,\n","       1000.0])\n","\n","from sklearn.model_selection import train_test_split\n","\n","train_input, test_input, train_target, test_target=train_test_split(perch_full, perch_weight, random_state=42)\n","```\n","\n","<br/>\n","\n","* 특성의 전처리\n","\n","```python\n","poly=PolynomialFeatures(include_bias=False)\n","#fit이 선행된 후 transform을 사용해야 함\n","poly.fit(train_input)\n","train_poly=poly.transform(train_input)\n","test_poly=poly.transform(test_input)\n","print(train_poly.shape)\n","```\n","    (42, 9)\n",":::info 변환기\n","* 특성의 전처리를 위한 클래스\n","* 특성을 서로 조합하여 여러 특성을 만들 수 있다.\n","* 위의 예제에선  ['x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2'] 이렇게 9개의 특성이 만들어짐(length, height, width)\n","* 이후 학습할 변환기의 degree를 이용하여 특성의 최대 제곱항을 정할 수 있다.\n",":::\n","\n","<br/>\n","\n","* 9개의 특성에 대한 모델 평가\n","\n","```python\n","from sklearn.linear_model import LinearRegression\n","\n","lr=LinearRegression()\n","lr.fit(train_poly, train_target)\n","print(lr.score(train_poly, train_target))\n","print(lr.score(test_poly, test_target))\n","```\n","    0.9903183436982124\n","    0.9714559911594134\n","\n","<br/>\n","\n","* 데이터 변환기의 조작을 통한 특성 조절\n","\n","```python\n","#degree: 5제곱항까지 만들어서 넣기\n","poly=PolynomialFeatures(degree=5, include_bias=False)\n","poly.fit(train_input)\n","train_poly=poly.transform(train_input)\n","test_poly=poly.transform(test_input)\n","#특성이 55개나 만들어짐\n","print(train_poly.shape)\n","```\n","    (42, 55)\n","\n","<br/>\n","\n","* 55개의 특성에 대한 모델 평가\n","\n","```python\n","lr.fit(train_poly, train_target)\n","#훈련 세트에 대해 거의 완벽하게 학습\n","print(lr.score(train_poly, train_target))\n","#훈련 세트에 너무 과대적합되어 테스트 세트는 점수가 매우 낮아짐\n","print(lr.score(test_poly, test_target))\n","```\n","    0.9999999999991097\n","    -144.40579242684848\n",":::warning 전처리에 대한 주의\n","* 무조건 특성의 개수가 많다고 해서 모델의 성능이 올라가지 않는다.\n","* 특성의 개수가 과도하게 많아지면 훈련 모델에 매우 과대적합 되어 general한 모델이 될 수 없다.\n","* 이후 배울 **규제**로 어느정도 해결 가능하다.\n",":::\n","\n","<br/>\n","\n","* 데이터의 규제를 위한 scale 정규화\n","\n","```python\n","from sklearn.preprocessing import StandardScaler\n","\n","ss=StandardScaler()\n","ss.fit(train_poly)\n","#데이터의 표준화\n","train_scaled=ss.transform(train_poly)\n","test_scaled=ss.transform(test_poly)\n","```\n",":::info 표준화를 하는 이유는?\n","* 정규화되지 않은 데이터는 선형 회귀 모델에서 곱해지는 계수값의 차이가 커지게 되며, 크기가 많이 차이날수록 공정하게 제어되지 못하기 때문\n",":::\n",":::warning 표준화 주의점\n","* 표준화를 진행할 때 train 데이터와 test데이터를 따로 훈련시켜 적용하면 안 된다.\n","* train 데이터에서의 표준편차와 test 데이터에서의 표준편차는 다르게 나올 것이므로, train 데이터로 통일하여 표준화를 진행하여야 한다.\n",":::\n","\n","<br/>\n","\n","* 릿지(ridge)를 이용한 규제\n","\n","```python\n","#릿지(ridge)\n","from sklearn.linear_model import Ridge\n","\n","ridge=Ridge()\n","ridge.fit(train_scaled, train_target)\n","#훈련세트에 너무 과대적합되지 않아 테스트 세트에서도 좋은 성능을 가짐\n","print(ridge.score(train_scaled, train_target))\n","print(ridge.score(test_scaled, test_target))\n","```\n","    0.9896101671037343\n","    0.9790693977615391\n",":::info 릿지(Ridge)\n","* 계수를 제곱한 값을 기준으로 규제를 적용한다.(일반적으로 사용)\n","* alpha값으로 규제의 정도를 조절하여, 규제를 강하게 하면 **과소적합**을 유도할 수 있다.\n",":::\n","\n","<br/>\n","\n","* alpha 값에 대한 릿지의 모델 평가\n","\n","```python\n","import matplotlib.pyplot as plt\n","\n","train_score=[]\n","test_score=[]\n","\n","#alpha값들\n","alpha_list=[0.001, 0.01, 0.1, 1, 10, 100]\n","\n","#ridge 객체를 각 alpha값에 대해 생성 후 훈련 및 모델 평가값을 배열에 추가 \n","for alpha in alpha_list:\n","    ridge=Ridge(alpha=alpha)\n","    ridge.fit(train_scaled, train_target)\n","    train_score.append(ridge.score(train_scaled, train_target))\n","    test_score.append(ridge.score(test_scaled, test_target))\n","```\n",":::info 하이퍼 파라미터란?\n","* 머신러닝 모델이 학습할 수 없고, 사람이 직접 알려주어야 하는 파라미터\n",":::\n","\n","<br/>\n","\n","* alpha 값에 따른 릿지 모델 평가 그래프\n","\n","```python\n","plt.plot(np.log10(alpha_list), train_score)\n","plt.plot(np.log10(alpha_list), test_score)\n","plt.xlabel('alpha')\n","plt.ylabel('R^2')\n","plt.show()\n","\n","#맨 왼쪽: 과대적합, 오른쪽으로 갈 수록 과소적합\n","```\n","![png](JungHun_files/JungHun1.png)\n","\n",":::info alpha 값에 대한 상용 로그의 의미\n","* 테스트로 주어진 alpha값들의 단위가 0.001~100까지 매우 다양하다.\n","* 그래프로 그릴 때 단위가 매우 촘촘해지기 때문에, 상용 로그를 취하여 지수만 표현한다.\n",":::\n","\n","<br/>\n","\n","* 최적의 alpha값에 대한 모델 평가\n","\n","```python\n","#테스트 세트의 점수가 가장 높고, 훈련 세트와 점수가 같은 0.1 선택\n","ridge=Ridge(alpha=0.1)\n","ridge.fit(train_scaled, train_target)\n","\n","#좋은 결과\n","print(ridge.score(train_scaled, train_target))\n","print(ridge.score(test_scaled, test_target))\n","```\n","    0.9903815817570365\n","    0.9827976465386884"],"metadata":{"id":"j_EcPryVot-b"}}]}